ML-Assignment-5-Clustering Algorithm

 Loading and Preprocessing (1 mark)
We start by loading the Iris dataset and dropping the 'species' column, as clustering does not rely on labeled data (the 'species' column is used for classification). We'll then prepare the dataset for clustering by extracting only the feature columns.

python
Copy
# Importing necessary libraries
from sklearn.datasets import load_iris
import pandas as pd

# Loading the Iris dataset
iris = load_iris()
X = iris.data  # Features only
X_df = pd.DataFrame(X, columns=iris.feature_names)

# Checking the first few rows of the dataset
X_df.head()
The dataset contains four features: sepal length, sepal width, petal length, and petal width.
We are not using the species column in this case since it's a clustering problem.
2. Clustering Algorithm Implementation (8 marks)
A) KMeans Clustering (4 marks)
How KMeans Works:
KMeans is an unsupervised machine learning algorithm that partitions data into K clusters.
It works by assigning each data point to the nearest cluster center, then recalculating the centroids (mean of points in the cluster) and repeating the process until convergence.
The number of clusters K must be predefined.
Why KMeans for Iris:
The Iris dataset consists of features that are likely to exhibit distinct groupings, making KMeans appropriate. It also scales well with the number of data points.
We can use KMeans to identify natural groupings within the dataset, even though the actual labels are not being used.
Implementation of KMeans Clustering:
python
Copy
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Apply KMeans clustering
kmeans = KMeans(n_clusters=3, random_state=42)  # Number of clusters = 3 (since Iris has 3 species)
kmeans.fit(X_df)

# Get the predicted labels for the clusters
y_kmeans = kmeans.predict(X_df)

# Visualizing the clusters (using the first two features for simplicity)
plt.scatter(X_df.iloc[:, 0], X_df.iloc[:, 1], c=y_kmeans, cmap='viridis')
plt.xlabel('Sepal Length')
plt.ylabel('Sepal Width')
plt.title('KMeans Clustering of Iris Dataset')
plt.show()
We chose K=3 since there are 3 species in the Iris dataset.
The scatter plot shows the data points colored by their cluster assignment.
B) Hierarchical Clustering (4 marks)
How Hierarchical Clustering Works:
Hierarchical clustering builds a tree-like structure (dendrogram) that groups data points into clusters.
There are two types: agglomerative (bottom-up) and divisive (top-down). Agglomerative is more commonly used.
In agglomerative hierarchical clustering, every data point starts as its own cluster, and pairs of clusters are merged as we move up the hierarchy based on their distance (e.g., Euclidean distance).
Why Hierarchical Clustering for Iris:
Hierarchical clustering doesnâ€™t require the number of clusters to be predefined. It can help visualize the data's hierarchical structure, showing how clusters relate to each other at different levels of similarity.
It is suitable for the Iris dataset to uncover different levels of grouping between the flowers.
Implementation of Hierarchical Clustering:
python
Copy
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage

# Apply Agglomerative Clustering
agg_clust = AgglomerativeClustering(n_clusters=3)  # Number of clusters = 3
y_agg = agg_clust.fit_predict(X_df)

# Visualizing the clusters
plt.scatter(X_df.iloc[:, 0], X_df.iloc[:, 1], c=y_agg, cmap='viridis')
plt.xlabel('Sepal Length')
plt.ylabel('Sepal Width')
plt.title('Hierarchical Clustering of Iris Dataset')
plt.show()

# Dendrogram for hierarchical clustering
linked = linkage(X_df, 'ward')
plt.figure(figsize=(10, 7))
dendrogram(linked)
plt.title('Dendrogram for Hierarchical Clustering')
plt.xlabel('Sample Index')
plt.ylabel('Distance')
plt.show()
AgglomerativeClustering performs bottom-up hierarchical clustering and we specify n_clusters=3.
We also plot a dendrogram to visualize the clustering hierarchy, showing how individual data points merge into clusters at different similarity levels.