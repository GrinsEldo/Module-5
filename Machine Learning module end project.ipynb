Machine Learning module end project

# Step 1: Importing necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Step 2: Load the dataset
# Replace this with the path to your dataset
df = pd.read_csv('car_prices.csv')

# Step 3: Data Preprocessing
# Display the first few rows of the dataset
print(df.head())

# Handling missing values (if any)
df = df.dropna()

# Encoding categorical variables if there are any
df = pd.get_dummies(df, drop_first=True)

# Split the dataset into features and target variable
X = df.drop(columns=['Price'])  # 'Price' is the target variable
y = df['Price']

# Split the dataset into training and testing sets (80/20 split)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 4: Model Implementation
# Initialize models
models = {
    "Linear Regression": LinearRegression(),
    "Decision Tree Regressor": DecisionTreeRegressor(random_state=42),
    "Random Forest Regressor": RandomForestRegressor(random_state=42),
    "Gradient Boosting Regressor": GradientBoostingRegressor(random_state=42),
    "Support Vector Regressor": SVR()
}

# Step 5: Model Training and Evaluation
model_results = {}

for model_name, model in models.items():
    # Train the model
    model.fit(X_train, y_train)
    
    # Make predictions on the test set
    y_pred = model.predict(X_test)
    
    # Calculate evaluation metrics
    r2 = r2_score(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    
    # Store the results
    model_results[model_name] = {
        "R-squared": r2,
        "Mean Squared Error": mse,
        "Mean Absolute Error": mae
    }

# Step 6: Display Model Performance
model_results_df = pd.DataFrame(model_results).T
print(model_results_df)

# Step 7: Feature Importance Analysis (For Tree-based models like Random Forest)
# Random Forest Feature Importance
rf_model = RandomForestRegressor(random_state=42)
rf_model.fit(X_train, y_train)

# Plotting feature importances
importances = rf_model.feature_importances_
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(10, 6))
plt.title('Feature Importance (Random Forest)')
plt.barh(range(X_train.shape[1]), importances[indices], align="center")
plt.yticks(range(X_train.shape[1]), X_train.columns[indices])
plt.xlabel('Relative Importance')
plt.show()

# Step 8: Hyperparameter Tuning (for Random Forest as an example)
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5, 10]
}

grid_search = GridSearchCV(RandomForestRegressor(random_state=42), param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)
grid_search.fit(X_train, y_train)

# Best parameters from Grid Search
print("Best Parameters: ", grid_search.best_params_)

# Train the best model
best_rf_model = grid_search.best_estimator_

# Predict using the best model
y_pred_best_rf = best_rf_model.predict(X_test)

# Evaluation on the best model
r2_best_rf = r2_score(y_test, y_pred_best_rf)
mse_best_rf = mean_squared_error(y_test, y_pred_best_rf)
mae_best_rf = mean_absolute_error(y_test, y_pred_best_rf)

print("\nPerformance of the best tuned Random Forest model:")
print(f"R-squared: {r2_best_rf:.4f}")
print(f"Mean Squared Error: {mse_best_rf:.4f}")
print(f"Mean Absolute Error: {mae_best_rf:.4f}")
Explanation:
Loading and Preprocessing (Step 2 & 3):

The dataset is loaded from a CSV file.
Missing values are handled (by dropping rows with missing data).
Categorical variables are encoded using one-hot encoding (if applicable).
Model Implementation (Step 4):

Five regression models are defined: Linear Regression, Decision Tree Regressor, Random Forest Regressor, Gradient Boosting Regressor, and Support Vector Regressor.
Model Training and Evaluation (Step 5):

Each model is trained and evaluated based on three metrics: R-squared, Mean Squared Error (MSE), and Mean Absolute Error (MAE).
Feature Importance Analysis (Step 7):

For tree-based models (like Random Forest), the feature importance is plotted, showing the relative importance of each feature in predicting the target variable (Price).
Hyperparameter Tuning (Step 8):

Hyperparameter tuning is performed using GridSearchCV for the Random Forest model. The best hyperparameters are identified, and the model is retrained and evaluated on the test set.
